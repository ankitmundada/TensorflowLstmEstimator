{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 07/11/2017\n",
    "Author: Ankit Mundada\n",
    "\n",
    "\n",
    "This is an implemetation of RNN in tensorflow. \n",
    "\n",
    "Dataset: Sample text corpus taken from http://www.taleswithmorals.com/. \n",
    "The corpus contains a vocabulary of only 112 different words. It can be extended to bigger size aswell.\n",
    "\n",
    "There are three different datasets. Train.txt, Val.txt and Test.txt\n",
    "Currently, all of them are the same, as the goal is not improve the model but understand the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from select import select\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "IS_DEBUG = True\n",
    "IS_GPU_AVL = False\n",
    "DO_PREDICTION = True\n",
    "\n",
    "# parameters to tune\n",
    "PARAMS = {\n",
    "    'NUM_ITER': 2 if IS_DEBUG else 50,\n",
    "    'STEPS_PER_ITER': 100,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'CONTEXT_SIZE': 3,  # context of words to predict next word\n",
    "    'LSTM_SIZE': 32,  # number of hidden units in LSTM cell\n",
    "    'BATCH_SIZE': 16,\n",
    "}\n",
    "\n",
    "FLAGS = {\n",
    "    'MODEL_DIR': './logs/rnn_1', # directory where model will be saved\n",
    "    'TRAINING_DATA': './datasets/train.txt',\n",
    "    'VAL_DATA': './datasets/val.txt',\n",
    "    'TEST_DATA': './datasets/test.txt'\n",
    "}\n",
    "\n",
    "vocab_size = utils.initiate_vocabs()\n",
    "output_size = vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This input funtion is an implemetation of a standard Input Pipeline using Tensorflow's Dataset API for a Text data.\n",
    "\n",
    "1. The input file has to be in csv format, with features at the start and labels at the end\n",
    "2. `TextLineDataset` reads csv row by row. Hence it does not load entire file into the memory at the same time.\n",
    "3. `decode_csv` preprocesses each row and returns a dictionary of `feature tensors` with feature name as the key and also a `target tensor`\n",
    "4. Different parameter line `repeat_count`, `shuffle`, `batch_size` make it easier to tweak with the function vary easily without affecting any other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filepath, mode=None):\n",
    "    \"\"\"\n",
    "    Implements the Recommended Input pipeline architecture of Tensorflow.\n",
    "    :param filepath: File to be loaded into memory line by line. (MUST be a CSV)\n",
    "    :param mode: One of the tf.estimator.ModeKeys (Train, Eval, Predict)\n",
    "    :return: The input features and target values for the current step\n",
    "    \"\"\"\n",
    "\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    repeat_count = None if is_training else 1\n",
    "\n",
    "    default_val = [[0.0] for _ in range(PARAMS['CONTEXT_SIZE'])]\n",
    "    default_val.append([0])  # output class should have data-type tf.int32, for accuracy calculations in model_fn\n",
    "\n",
    "    def decode_csv(line):\n",
    "        line = tf.decode_csv(line, default_val)\n",
    "        return {'context': line[:-1]}, line[-1]\n",
    "\n",
    "    dataset = tf.contrib.data\\\n",
    "        .TextLineDataset(utils.make_csv(filepath, PARAMS['CONTEXT_SIZE']))\\\n",
    "        .map(decode_csv, num_threads=4 if IS_GPU_AVL else 2)  # preprocessing\n",
    "    # shuffle input\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=PARAMS['BATCH_SIZE'] * 2)\n",
    "    dataset = dataset.repeat(repeat_count)\n",
    "    dataset = dataset.batch(PARAMS['BATCH_SIZE'])\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature, next_label = iterator.get_next()\n",
    "    return next_feature, next_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart of the Estimator that we will be building next. This function will take as input `features` and `labels` returned from `input_fn` above and enque them into the Graph that we create. \n",
    "\n",
    "1. Based on the value of `mode`, the LSTM model performs different operations like traing, evaluation or inference.\n",
    "2. Model architechture can easily be modified here, for ex by adding a new `LSTMCell` or adding `Dropout` or something else\n",
    "3. The funtion returns an `EstimatorSpec` object, whose contensts depend on the `mode`\n",
    "4. To view diferent types summaries in `Tensorboard` or on the `console` or to write to a file, different kind of hooks can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"\n",
    "    Required to be passed into the Estimator. This function model a simple LSTM model using Tensorflow's in-built and\n",
    "    efficient implementations of LSTM cell\n",
    "    :param features: features as returned from input_fn\n",
    "    :param labels: labels as returned from input_fn\n",
    "    :param mode: mode set by the different method calls of the Estimator Object\n",
    "    :param params: params passed with the Estimator\n",
    "    :return: returns an EstimatorSpec which store's different important params to analyze tha model\n",
    "    \"\"\"\n",
    "    # break the context words aka features into 'time-steps'\n",
    "    x = tf.split(features['context'], PARAMS['CONTEXT_SIZE'], 1)\n",
    "\n",
    "    # setting up LSTM\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(PARAMS['LSTM_SIZE'])\n",
    "    output, _ = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # only last time-step matters\n",
    "    output = output[-1]\n",
    "\n",
    "    # get linear predictions from LSTM outputs\n",
    "    weight = tf.Variable(tf.random_normal([PARAMS['LSTM_SIZE'], output_size]))\n",
    "    bias = tf.Variable(tf.random_normal([output_size]))\n",
    "    logits = tf.matmul(output, weight) + bias\n",
    "\n",
    "    # final predictions\n",
    "    preds = tf.argmax(tf.nn.softmax(logits), axis=1, output_type=tf.int32)\n",
    "    preds_dict = {\"predictions\": preds}\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=preds_dict,\n",
    "        )\n",
    "\n",
    "    # using a Cross-Entropy error for this muticlass classification problem\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # setting up different metrices to be monitored using tensorboard\n",
    "        eval_metric_ops = {\n",
    "            'accuracy': tf.metrics.accuracy(labels=labels, predictions=preds),\n",
    "            'precision': tf.metrics.precision(labels=labels, predictions=preds),\n",
    "            'recall': tf.metrics.recall(labels=labels, predictions=preds)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=preds_dict,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops\n",
    "        )\n",
    "\n",
    "    correct_pred = tf.equal(preds, labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    # creating a summary to be added to SummarySaveHook\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # define the training operation/optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=PARAMS['LEARNING_RATE'])\n",
    "    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    tensors_to_save = {'training_accuracy': 'accuracy'}\n",
    "    # Hooks to monitor the performance of the model during training\n",
    "    hook_logging = tf.train.LoggingTensorHook(tensors_to_save, every_n_iter=25)\n",
    "    hook_summary = tf.train.SummarySaverHook(save_steps=25, output_dir=FLAGS['MODEL_DIR'], scaffold=None,\n",
    "                                             summary_op=tf.summary.merge_all())\n",
    "\n",
    "    # return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=preds_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        training_hooks=[hook_logging, hook_summary]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize estimator. it'll automatically load the most recent saved model in the 'model_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './logs/rnn_1', '_log_step_count_steps': 100, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_tf_random_seed': 1, '_keep_checkpoint_max': 5}\n"
     ]
    }
   ],
   "source": [
    "rnn_regressor = tf.estimator.Estimator(model_fn=model_fn, model_dir=FLAGS['MODEL_DIR'], params=PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model. In the debug mode, it only runs for a single step and a single iteration. This parameters can be changed at the beginning of this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./logs/rnn_1/model.ckpt-2050\n",
      "INFO:tensorflow:Saving checkpoints for 2051 into ./logs/rnn_1/model.ckpt.\n",
      "INFO:tensorflow:step = 2051, loss = 1.40404\n",
      "INFO:tensorflow:training_accuracy = 0.6875\n",
      "INFO:tensorflow:Saving checkpoints for 2052 into ./logs/rnn_1/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.17907.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-08-02:05:56\n",
      "INFO:tensorflow:Restoring parameters from ./logs/rnn_1/model.ckpt-2052\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-08-02:05:57\n",
      "INFO:tensorflow:Saving dict for global step 2052: accuracy = 0.26, global_step = 2052, loss = 2.45616, precision = 0.95288, recall = 0.973262\n",
      "Evaluation loss after 0 iters is: 2.456157\n",
      "Stop training now?\n",
      "Type y for Yes\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./logs/rnn_1/model.ckpt-2052\n",
      "INFO:tensorflow:Saving checkpoints for 2053 into ./logs/rnn_1/model.ckpt.\n",
      "INFO:tensorflow:step = 2053, loss = 1.37682\n",
      "INFO:tensorflow:training_accuracy = 0.6875\n",
      "INFO:tensorflow:Saving checkpoints for 2054 into ./logs/rnn_1/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.1518.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-08-02:05:57\n",
      "INFO:tensorflow:Restoring parameters from ./logs/rnn_1/model.ckpt-2054\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-08-02:05:57\n",
      "INFO:tensorflow:Saving dict for global step 2054: accuracy = 0.25, global_step = 2054, loss = 2.47686, precision = 0.953125, recall = 0.97861\n",
      "Evaluation loss after 1 iters is: 2.476856\n",
      "Stop training now?\n",
      "Type y for Yes\n"
     ]
    }
   ],
   "source": [
    "TIMEOUT = 5  # wait after each iter for 5 sec, to quit training the model, as it may require more tuning at that stage\n",
    "for i in range(PARAMS['NUM_ITER']):\n",
    "    rnn_regressor.train(input_fn=lambda: input_fn(FLAGS['TRAINING_DATA'], mode=tf.estimator.ModeKeys.TRAIN),\n",
    "                        steps=2 if IS_DEBUG else PARAMS['STEPS_PER_ITER'])\n",
    "\n",
    "    # Evaluate at the end of each iteration\n",
    "    eval_results = rnn_regressor.evaluate(input_fn=lambda : input_fn(FLAGS['VAL_DATA'], mode=tf.estimator.ModeKeys.EVAL))\n",
    "    print('Evaluation loss after %d iters is: %f' % (i, eval_results['loss']))\n",
    "\n",
    "    # Stop training in between in more tuning is required.\n",
    "    print(\"Stop training now?\\nType y for Yes\")\n",
    "    rlist = select([sys.stdin], [], [], TIMEOUT)[0]\n",
    "    feedback = None\n",
    "    if rlist:\n",
    "        feedback = sys.stdin.readline().strip()\n",
    "        if (feedback is 'y') or (feedback is 'yes'):\n",
    "            print('Finishing the training')\n",
    "            break\n",
    "    else:\n",
    "        print('Training for the next iteration')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on the test dataset!\n",
    "\n",
    "In each sample output, the first three words are the input given to the model and the last (ie fourth) word is the one _predicted_ by the model. \n",
    "\n",
    "Note: The can be tweaked a lot. This is only an implemetation exmple. The test-set outputs are not great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ./logs/rnn_1/model.ckpt-2054\n",
      "0   long ago , the\n",
      "1   ago , the mice\n",
      "2   , the mice had\n",
      "3   the mice had a\n",
      "4   mice had a general\n",
      "5   had a general to\n",
      "6   a general council to\n",
      "7   general council to consider\n",
      "8   council to consider what\n",
      "9   to consider what danger\n",
      "10   consider what measures when\n",
      "11   what measures they could\n",
      "12   measures they could consider\n",
      "13   they could take to\n",
      "14   could take to to\n",
      "15   take to outwit what\n",
      "16   to outwit their the\n",
      "17   outwit their common the\n",
      "18   their common enemy got\n",
      "19   common enemy , cat\n",
      "20   enemy , the cat\n",
      "21   , the cat had\n",
      "22   the cat . some\n",
      "23   cat . some said\n",
      "24   . some said this\n",
      "25   some said this .\n",
      "26   said this , and\n",
      "27   this , and some\n",
      "28   , and some had\n",
      "29   and some said this\n",
      "30   some said that .\n",
      "31   said that but at\n",
      "32   that but at last\n",
      "33   but at last ,\n",
      "34   at last a young\n",
      "35   last a young to\n",
      "36   a young mouse got\n",
      "37   young mouse got up\n",
      "38   mouse got up last\n",
      "39   got up and said\n",
      "40   up and said that\n",
      "41   and said he had\n",
      "42   said he had a\n",
      "43   he had a general\n",
      "44   had a proposal to\n",
      "45   a proposal to proposal\n",
      "46   proposal to make .\n",
      "47   to make , you\n",
      "48   make , which .\n",
      "49   , which he said\n",
      "50   which he thought to\n",
      "51   he thought would .\n",
      "52   thought would meet mouse\n",
      "53   would meet the the\n",
      "54   meet the case .\n",
      "55   the case . to\n",
      "56   case . you .\n",
      "57   . you will .\n",
      "58   you will all got\n",
      "59   will all agree to\n",
      "60   all agree , until\n",
      "61   agree , said mice\n",
      "62   , said he had\n",
      "63   said he , and\n",
      "64   he , that some\n",
      "65   , that our measures\n",
      "66   that our chief the\n",
      "67   our chief danger mouse\n",
      "68   chief danger consists when\n",
      "69   danger consists in the\n",
      "70   consists in the sly\n",
      "71   in the sly .\n",
      "72   the sly and to\n",
      "73   sly and treacherous mouse\n",
      "74   and treacherous manner old\n",
      "75   treacherous manner in the\n",
      "76   manner in which got\n",
      "77   in which the cat\n",
      "78   which the enemy .\n",
      "79   the enemy approaches .\n",
      "80   enemy approaches us .\n",
      "81   approaches us . the\n",
      "82   us . now .\n",
      "83   . now , you\n",
      "84   now , if .\n",
      "85   , if we to\n",
      "86   if we could receive\n",
      "87   we could receive a\n",
      "88   could receive some got\n",
      "89   receive some signal to\n",
      "90   some signal of ,\n",
      "91   signal of her got\n",
      "92   of her approach ,\n",
      "93   her approach , the\n",
      "94   approach , we could\n",
      "95   , we could some\n",
      "96   we could easily last\n",
      "97   could easily escape a\n",
      "98   easily escape from to\n",
      "99   escape from her to\n",
      "100   from her . the\n",
      "101   her . i .\n",
      "102   . i venture the\n",
      "103   i venture , the\n",
      "104   venture , therefore .\n",
      "105   , therefore , you\n",
      "106   therefore , to could\n",
      "107   , to propose had\n",
      "108   to propose that proposal\n",
      "109   propose that a proposal\n",
      "110   that a small a\n",
      "111   a small bell got\n",
      "112   small bell be to\n",
      "113   bell be procured ,\n",
      "114   be procured , the\n",
      "115   procured , and mice\n",
      "116   , and attached .\n",
      "117   and attached by a\n",
      "118   attached by a ribbon\n",
      "119   by a ribbon what\n",
      "120   a ribbon round the\n",
      "121   ribbon round the the\n",
      "122   round the neck .\n",
      "123   the neck of the\n",
      "124   neck of the the\n",
      "125   of the cat .\n",
      "126   the cat . some\n",
      "127   cat . by said\n",
      "128   . by this a\n",
      "129   by this means to\n",
      "130   this means we to\n",
      "131   means we should to\n",
      "132   we should always common\n",
      "133   should always know the\n",
      "134   always know when mouse\n",
      "135   know when she got\n",
      "136   when she was up\n",
      "137   she was about ,\n",
      "138   was about , the\n",
      "139   about , and mice\n",
      "140   , and could had\n",
      "141   and could easily had\n",
      "142   could easily retire a\n",
      "143   easily retire while ,\n",
      "144   retire while she got\n",
      "145   while she was up\n",
      "146   she was in up\n",
      "147   was in the neighbourhood\n",
      "148   in the neighbourhood .\n",
      "149   the neighbourhood . this\n",
      "150   neighbourhood . this proposal\n",
      "151   . this proposal a\n",
      "152   this proposal met ,\n",
      "153   proposal met with when\n",
      "154   met with general the\n",
      "155   with general applause mouse\n",
      "156   general applause , the\n",
      "157   applause , until .\n",
      "158   , until an the\n",
      "159   until an old the\n",
      "160   an old mouse got\n",
      "161   old mouse got up\n",
      "162   mouse got up last\n",
      "163   got up and said\n",
      "164   up and said that\n",
      "165   and said that some\n",
      "166   said that is at\n",
      "167   that is all at\n",
      "168   is all very to\n",
      "169   all very well ,\n",
      "170   very well , the\n",
      "171   well , but .\n",
      "172   , but who .\n",
      "173   but who is to\n",
      "174   who is to receive\n",
      "175   is to bell a\n",
      "176   to bell the this\n",
      "177   bell the cat .\n",
      "178   the cat ? a\n",
      "179   cat ? the outwit\n",
      "180   ? the mice .\n",
      "181   the mice looked a\n",
      "182   mice looked at to\n",
      "183   looked at one what\n",
      "184   at one another when\n",
      "185   one another and the\n",
      "186   another and nobody mouse\n",
      "187   and nobody spoke the\n",
      "188   nobody spoke . the\n",
      "189   spoke . then .\n",
      "190   . then the which\n",
      "191   then the old .\n",
      "192   the old mouse a\n",
      "193   old mouse said neighbourhood\n",
      "194   mouse said it .\n",
      "195   said it is to\n",
      "196   it is easy to\n",
      "197   is easy to young\n",
      "198   easy to propose to\n",
      "199   to propose impossible to\n"
     ]
    }
   ],
   "source": [
    "if DO_PREDICTION:\n",
    "    # get results on the test data after all the training\n",
    "    predictions = rnn_regressor.predict(input_fn=lambda: input_fn(FLAGS['TEST_DATA'], mode=tf.estimator.ModeKeys.PREDICT))\n",
    "    with open(utils.make_csv(FLAGS['TEST_DATA'], PARAMS['CONTEXT_SIZE']), 'r') as infile:\n",
    "        for i, pred in enumerate(predictions):\n",
    "            list_idx = infile.readline().strip().split(sep=',')[:-1] + [pred['predictions'].tolist()]\n",
    "            pred_word = utils.convert_index_to_word(list_idx)\n",
    "            print(i, ' ', ' '.join(pred_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the entire code in this file: Please checkout __simple_rnn.py__ from _Jupyter Home_ \n",
    "Thank you. Please let me know of your feedbacks and sugesstions at ankit.mundada93@gmail.com or at +91-7076607420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
